import {Injectable, EventEmitter, Input, Output} from "angular2/core";

@Injectable()
export class SpeechService {

    @Output() onSpeaking: EventEmitter<string>;
    @Output() onError: EventEmitter<string>;
    @Output() onRecognized: EventEmitter<string>;
    @Output() onStatusChanged: EventEmitter<string>;

    recognizer:any;
    // localization resources
    context: any;
    resourceMap: any;

    constructor() {
        this.onSpeaking = new EventEmitter();
        this.onError = new EventEmitter();
        this.onRecognized = new EventEmitter();
        this.onStatusChanged = new EventEmitter();
        
        if (typeof (Windows) != "undefined") {
            let defaultLang = Windows.Media.SpeechRecognition.SpeechRecognizer.systemSpeechLanguage;
            let rcns = Windows.ApplicationModel.Resources.Core;
            this.context = new rcns.ResourceContext();
            this.context.languages = new Array(defaultLang.languageTag);
            this.resourceMap = rcns.ResourceManager.current.mainResourceMap.getSubtree('LocalizationSpeechResources');
            this.initializeRecognizer(defaultLang);
        }
    }
    
    say(speechString: string) {
        if (typeof (Windows) != "undefined") {

            let synth = Windows.Media.SpeechSynthesis.SpeechSynthesizer();

            let audio = new Audio(); //creating an audio object

            this.onSpeaking.emit(speechString);
            
            console.log("Speaking the phrase: " + speechString);

            
            
            // get all voices
            let allVoices = Windows.Media.SpeechSynthesis.SpeechSynthesizer.allVoices;
            // Find the right voice for now, just use index 0  - Female is allVoices[n].gender=1 with language "en-US".  displayName = "Microsoft Zira Mobile"
            let selectedVoice = allVoices[0];

            // and use that voice, to be set
            synth.voice = selectedVoice;
            synth.synthesizeTextToStreamAsync(speechString).then(function(markersStream) {
                // Convert the stream to a URL Blob.
                let blob = MSApp.createBlobFromRandomAccessStream(markersStream.ContentType, markersStream);

                // Send the Blob to the audio object.
                audio.src = URL.createObjectURL(blob, { oneTimeOnly: true });
                markersStream.seek(0); // start at beginning when speak is hit
       
                audio.play();

            });
        }
    }

    initializeRecognizer(language) {
        /// <summary>
        /// Initialize speech recognizer and compile constraints.
        /// </summary>
        if (typeof this.recognizer !== 'undefined') {
            this.recognizer = null;
        }
        this.recognizer = Windows.Media.SpeechRecognition.SpeechRecognizer(language);

        // Provide feedback to the user about the state of the recognizer.
        this.recognizer.addEventListener('statechanged', this.onSpeechRecognizerStateChanged, false);
        // Handle continuous recognition events. Completed fires when various error states occur or the session otherwise ends.
        // ResultGenerated fires when recognized phrases are spoken or the garbage rule is hit.
        this.recognizer.continuousRecognitionSession.addEventListener('resultgenerated', this.onSpeechRecognizerResultGenerated, false);
        this.recognizer.continuousRecognitionSession.addEventListener('completed', this.onSpeechRecognizerSessionCompleted, false);

        // Build a command-list grammar. Multiple commands can be given the same tag, allowing for variations on the same command to be handled easily.
        this.recognizer.constraints.append(
            Windows.Media.SpeechRecognition.SpeechRecognitionListConstraint([
                this.resourceMap.getValue('ListGrammarGoHome', this.context).valueAsString
            ], "GoHome"));
        this.recognizer.constraints.append(
            Windows.Media.SpeechRecognition.SpeechRecognitionListConstraint([
                this.resourceMap.getValue('ListGrammarShowWeather', this.context).valueAsString
            ], "ShowWeather"));
        this.recognizer.constraints.append(
            Windows.Media.SpeechRecognition.SpeechRecognitionListConstraint([
                this.resourceMap.getValue('DamnDaniel', this.context).valueAsString
            ], "DamnDaniel"));


        // RecognizeWithUIAsync allows developers to customize the prompts.
        var helpString = "Try saying '" +
            this.resourceMap.getValue('ListGrammarGoHome', this.context).valueAsString + "', '" +
            this.resourceMap.getValue('ListGrammarShowWeather', this.context).valueAsString + "' or '" +
        

            this.recognizer.compileConstraintsAsync().done(
                function(result) {
                    // Check to make sure that the constraints were in a proper format and the recognizer was able to compile them.
                    if (result.status != Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.success) {
                        // btnContinuousReco.disabled = true;
                        // Let the user know that the grammar didn't compile properly.
                        this.speechRecognizerUnsuccessful(result.status);
                    }
                    else {
                        // btnContinuousReco.disabled = false;
                    }
                }
            );

        this.continuousRecognition();
        this.say("I'm listening");
    }


    continuousRecognition() {
        /// <summary>
        /// Begin recognition or finish the recognition session.
        /// </summary>
        
        if (this.recognizer.state != Windows.Media.SpeechRecognition.SpeechRecognizerState.idle) { // Check if the recognizer is listening or going into a state to listen.

            this.recognizer.continuousRecognitionSession.stopAsync();
            return;
        }


        // Start the continuous recognition session. Results are handled in the event handlers below.
        try {
            this.recognizer.continuousRecognitionSession.startAsync().then(function completed() {
            });
        }
        catch (e) { }
    }

    onSpeechRecognizerResultGenerated(eventArgs) {
        /// <summary>
        /// Handle events fired when a result is generated. This may include a garbage rule that fires when general room noise
        /// or side-talk is captured (this will have a confidence of rejected typically, but may occasionally match a rule with
        /// low confidence).
        /// </summary>
        
        console.log("Speech Recognizer Results: " + eventArgs.result.constraint);
        
        
        // The garbage rule will not have a tag associated with it, the other rules will return a string matching the tag provided
        // when generating the grammar.
        var tag = "unknown";
        if (eventArgs.result.constraint != null) {
            tag = eventArgs.result.constraint.tag;
        }

        // Developers may decide to use per-phrase confidence levels in order to tune the behavior of their 
        // grammar based on testing.
        if (eventArgs.result.confidence == Windows.Media.SpeechRecognition.SpeechRecognitionConfidence.high ||
            eventArgs.result.confidence == Windows.Media.SpeechRecognition.SpeechRecognitionConfidence.medium) {
            
            this.onRecognized.emit(tag);     
        }
        else {
            this.onRecognized.emit(tag);
            console.log("sorry I didnt catch that")
        }
    }

    onSpeechRecognizerSessionCompleted(eventArgs) {
        /// <summary>
        /// Handle events fired when error conditions occur, such as the microphone becoming unavailable, or if
        /// some transient issues occur. This also fires when the session completes normally.
        /// </summary>
        if (eventArgs.status != Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.success) {
            this.speechRecognizerUnsuccessful(eventArgs.status);
        }

    }


    onSpeechRecognizerStateChanged(eventArgs) {
        /// <summary>
        /// Looks up the state text and displays the message to the user.
        /// </summary>
        
        console.log("Speech Recognizer State Changed: " + eventArgs);
        
        switch (eventArgs.state) {
            case Windows.Media.SpeechRecognition.SpeechRecognizerState.idle: {
                console.log("Speech recognizer state: idle");
                this.onStatusChanged.emit("idle");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognizerState.capturing: {
                //this.onError.emit("Speech recognizer state: capturing");
                console.log("Speech recognizer state: capturing");
                 this.onStatusChanged.emit("capturing");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognizerState.processing: {
                //this.onError.emit("Speech recognizer state: processing");
                console.log("Speech recognizer state: processing");
                 this.onStatusChanged.emit("processing");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognizerState.soundStarted: {
                //this.onError.emit("Speech recognizer state: soundStarted");
                console.log("Speech recognizer state: soundStarted");
                 this.onStatusChanged.emit("soundStarted");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognizerState.soundEnded: {
                //this.onError.emit("Speech recognizer state: soundEnded");
                console.log("Speech recognizer state: soundEnded");
                 this.onStatusChanged.emit("soundEnded");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognizerState.speechDetected: {
                //this.onError.emit("Speech recognizer state: speechDetected");
                console.log("Speech recognizer state: speechDetected");
                 this.onStatusChanged.emit("speechDetected");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognizerState.paused: {
                //this.onError.emit("Speech recognizer state: paused");
                console.log("Speech recognizer state: paused");
                 this.onStatusChanged.emit("paused");
                break;
            }
            default: {
                break;
            }
        }
    }

    speechRecognizerUnsuccessful(resultStatus) {
        /// <summary>
        /// Looks up the error text and displays the message to the user.
        /// </summary>
        console.debug("speechRecognizer Unsuccessful: " + resultStatus);
    
        switch (resultStatus) {
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.audioQualityFailure: {
                console.log("Speech recognition error: audioQualityFailure");
                this.onError.emit("audioQualityFailure");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.grammarCompilationFailure: {
                console.log("Speech recognition error: grammarCompilationFailure");
                 this.onError.emit("grammarCompilationFailure");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.grammarLanguageMismatch: {
                this.onError.emit("grammarLanguageMismatch");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.microphoneUnavailable: {
                this.onError.emit("microphoneUnavailable");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.networkFailure: {
                this.onError.emit("networkFailure");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.pauseLimitExceeded: {
                this.onError.emit("pauseLimitExceeded");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.timeoutExceeded: {
                this.onError.emit("timeoutExceeded");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.topicLanguageNotSupported: {
                this.onError.emit("topicLanguageNotSupported");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.unknown: {
                 this.onError.emit("unknown");
                break;
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionResultStatus.userCanceled: {
                 this.onError.emit("userCanceled");
                break;
            }
            default: {
                break;
            }
        }
    }

    convertConfidenceToString(confidence:string) {
        /// <summary> Converts numeric confidence value into text representation of
        /// Windows.Media.SpeechRecognition.SpeechRecognitionConfidence for visualization.
        /// <param name="confidence">The numeric confidence returned by SpeechRecognitionResult.Confidence</param>
        /// </summary>
             
        switch (confidence) {
            case Windows.Media.SpeechRecognition.SpeechRecognitionConfidence.high: {
                return "high";
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionConfidence.medium: {
                return "medium";
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionConfidence.low: {
                return "low";
            }
            case Windows.Media.SpeechRecognition.SpeechRecognitionConfidence.rejected: {
                return "rejected";
            }
            default:
                return "unknown";
        }
    }

};